----------------------------------------------------------------
Things to-do:
1. Fix the filterType. String value can be of type number and so on.
2. StartRow and endrow can be negative as of now for error screens(except error code counts). Fix that **** Create minor bug ticket
3. Filter Type, add a check for Type enum.
------------------------------------------------------
07-30-2020
Automation CAIS Add:
1. Don't like the validation on CAIS for dates. Repetetive
2. Effective date message wrong     "message": "The field \"effectiveDate\" 2020-07-29 must be greater than or equal to the current date",
	When in fact the current date is allowed
----------------------------------------------------------------
07-27-2020 release :
1. application.yml review ***
2. add the fip pipeline as the very first step in the VID ***
3. update the release/2.0.0 in f3 for build numbers ***

Steps:
1. Update config files ***

----------------------------------------------------------------
on for qa-int, and off for qa REPORT CARD FEATURE

Interfirm Linkage :
1. What db to read from ? *** catco database. We have same table in catdd db only for testing purpose
2. getStatisticsDetail refactor? why need to set = 0 and all ? Himanshu *** Its ok
3. Get one resultset for Interfirm and filter by subtype? *** Sure
4. Do we show null catReporterImid ? *** Imid has not null constraint
5. Add coverage for daily report card dao	***
6. Add javadoc to cache repository	***
7. Null if empty. Not 0s ???
8. Ask himanshu why duplicate unit test in StatsImpl ?

		linkerStatisticsInterFirmSentEntityList = new ArrayList<>();
        linkerStatisticsInterFirmSentEntityList.add(new Object[]{"JPMS", 90012L, "ERROR", "KEY DUPE", 105L, false, 0L, "E"});
        linkerStatisticsInterFirmSentEntityList.add(new Object[]{"JPMS", 90012L, "ATTEMPTED", "ATTEMPTED", 16L, true, 0L, "O"});
        linkerStatisticsInterFirmSentEntityList.add(new Object[]{"JPMX", 90012L, "ERROR", "UNLINKED", 20L, true, 0L, "O"});
        linkerStatisticsInterFirmSentEntityList.add(new Object[]{"JPMS", 90012L, "WARNING", "ACCEPTED WARNING", 31L, true, 0L, "O"});
        linkerStatisticsInterFirmSentEntityList.add(new Object[]{"JPMX", 90012L, "WARNING", "REJECTED WARNING", 42L, true, 0L, "O"});
		
		
		linkerStatisticsEntityList = new ArrayList<>();
        linkerStatisticsEntityList.add(new Object[] {"JPMS1", 1L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS1", 1L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS2", 2L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS2", 2L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS2", 2L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS3", 3L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS3", 3L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS3", 3L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS3", 3L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS4", 4L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS4", 4L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS4", 4L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS4", 4L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS4", 4L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS5", 5L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM SENT"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
        linkerStatisticsEntityList.add(new Object[] {"JPMS6", 6L, "INTERFIRM RECEIVED"});
--------------------------------
IM Report card EXPORT:
1. Export zip or csv ?	*** zip to be consistent with other APIs
2. Export file name ? ***
3. Api uri /reportcards/daily/files.zip correct? *** yes
4. Do we select a month(period) and export daily report for that month ? if not provided latest period ? *** yes
5. how we getting for a month ? how to calculate the last date ? ***
			use Date.from(period.atDay(1).atStartOfDay().atZone(ZoneId.systemDefault()).toInstant()); ***
6. SETUP A KICKOFF MEETING. ***
7. Add a version to the query param ? *** Yes
8. Should we move to catco dto package ? *** No its only for catco db, not catco schema in catdd db
9. Make period mandatory ? ***
10. Return 404 for empty

Test cases:
1. Invalid format YearMonth ? *** 400
2. Required field not sent ? Every single field ? *** 400
3. Empty Csv for month of may maybe ? Do we need the header atleast? *** Yes just headers for empty
4. Response : for July only Equities ? ***
5. Response : for August everything ? ***
6. FeatureFlag ?	***
7. Do we need to move this exportWriter into a different package ? currently in eventDaoHelper ? No ***
8. valid filename ? ***


Two comments:
If just to validate the query param date, why append a date and then check it? why not use YearMonth from DateTime java 8 package ? ***
Move produces to method level so that other methods don't have to use it. ***
--------------------------------
Preferences PATCH:
1. Do we need a previousId column to keep track of old records if updated? ***No
2. Enum for preference value ? Only JSON and CSV ? ***Choice lookup table created
3. Preference name should it be used from db or accept anything ?	*** From table

Steps:
1. Create entities out of the request.
	If found already exists : Find by checking orgId, preferenceName, activeFlag -> Update 
	-> If preserve record : get existing one; set existing one activeFlag to false;  new entity save
	-> If update directly : find existing, set values : save
	
	If not found : create an entity : save

Scenarios:
1. Pass multiple same preference name ? 400 *** What names should message contain ? ***->
2. Invalid preference name ? 400	***->
3. Invalid value ? 400	***	->
4. Single update ? 200	***	->
5. Multiple valid update ? json with values	***	->
6. Preference name/value Null values ? 400 ***	->
7. Empty request. Null request ?	***	->
8. Constraint violated?
9. DB has no records in name table ?	***-> 400 names not found
10. DB has no records in choice table ? *** 400 if the choice is invalid for preferences that has some data. For some that don't have anthing. accept
	any value *** ->
11. DB has no records in preference table ? *** create one if name and choice table has it	->	***
12. If no choices are available for any preferences, accept any values ?	***->
13. localhost:8080/api/v1/preferences/organizations/7009 -> New org which has no preference setup. With update, save the object ?
	{
    "preferences":
    [
        {
          "name": "namedFeedbackFileFormat2",
          "value": "JSON"
        },
         {
          "name": "namedFeedbackFileFormat",
          "value": "CSV"
        }
    ]
	}


//                if (!preferenceNameEntityMap.values().stream().map(PreferenceNameEntity::getPreferenceName).collect(Collectors.toList()).stream()
//                    .anyMatch(o -> Objects.equals(o, preferenceDetailRequest.getName())))
//                {
//                    hasValidPreferenceName = false;
//                }

        // check for npe
//        if (updatedPreferenceEntityList.size() != preferenceUpdateRequest.getPreferences().size())
//
//        if (!hasValidPreferenceName)
//        {
//            List<String> preferenceNameList =
//                preferenceUpdateRequest.getPreferences().stream().map(PreferenceDetailRequest::getName).collect(Collectors.toList());
//            throw new ObjectNotFoundException(String.format(PREFERENCE_NAME_NOT_FOUND, StringUtils.join(preferenceNameList, ", ")));
//        }
        // change cache to 10 min
//        if (!hasValidPreferenceValue)
//        {
//            List<String> preferenceValueList =
//                preferenceUpdateRequest.getPreferences().stream().map(PreferenceDetailRequest::getValue).collect(Collectors.toList());
//            throw new ObjectNotFoundException(String.format(INVALID_PREFERENCES_VALUES, StringUtils.join(preferenceValueList, ", ")));
//        }

--------------------------------

Preferences GET:
1. What is the expected behaviour if the api call has query params with preference names that do not belong to the org?
2. Tests:
	1. Isso Endpoint with no org Id or no query param should get all preferences for all the orgId.	
	2. Ews endpoint : no names provided. Should fetch all the preferences for provided org	***	---
					  valid names but not for provided org : Original Submission Format	*** ----
					  One valid one invalid name : 404	*** ---
					  One invalid, one valid but not set : 404	*** ---
					  One valid set, and one valid but not set for org : One Original Submission Format, one valid	*** ---
					  multiple valid	*** ---
	3. Ews endpoint : for orgs that has no data. 
					  no names provided : 	Get all the preference in lookup table with default value	*** ---
					  names provided: Name is valid but no preferences set, give Original Submission Format	for provided name	*** ----
					  Multiple names : Name is valid but no preferences set, give Original Submission Format for all names provided	*** ---
					  with invalid names query param:  404	***---
					  same names multiple times, only one response ? Only one	***---
					  same names multiple times default, only one response ? Only one	***---
3. check if cache is working ***
4. Default value : Original Submission Format	***

New tests:
1. If three names passed, one is valid but not set, and if the supportDefaultValue=false, set it all as null except name.
2. Audit info update. Set the whole thing as null for Default values.	***

DB has records:
1. localhost:8080/api/v1/preferences/organizations 											-> get all for all the orgs
2. localhost:8080/api/v1/preferences/organizations?supportDefaultValue=true					-> 400
3. localhost:8080/api/v1/preferences/organizations?names=namedFeedbackFileFormat			-> get the org associated with this name
4. localhost:8080/api/v1/preferences/organizations?names=namedFeedbackFileFormat,			-> get the orgs associated with these names
	namedFeedbackFileFormat3
	
	
	
DB has no records:														supportDefaultValue=false						supportDefaultValue=true
1. localhost:8080/api/v1/preferences/organizations						Everything with null values								400
2. localhost:8080/api/v1/preferences/organizations/79					Everything with null values					Everything with Default values
3. /preferences/organizations/79?names=namedFeedbackFileFormat			get 1 with null values						
4. /preferences/organizations/79?names=namedFeedbackFileFormat&supportDefaultValue=true																		
	
	
	
	* If No preferences set. But wanting to get the preferences, get default for everything in the table ?????
	* If Preferences set 2, Should we get 2 set preferences and rest regular?
	localhost:8080/api/v1/preferences/organizations/79/?supportDefaultValue=true
	
	------------------------- For ISSO, where supportDefaultValue=false
	If names are NOT provided, empty list if preference table is empty.
	If names are NOT provided, and only some are found, only return those, not the null for remaining ones.
	
	If names are provided, and nothing found null value for those ?
	If names are provided, and only some are found -> Set found ones to actual value, but not found ones(which are passed) need to be null
	
	
	localhost:8080/api/v1/preferences/organizations?supportDefaultValue=false&names=namedFeedbackFileFormat,namedFeedbackFileFormat2
	namedFeedbackFileFormat is in the preference table. In this case, what should be returned ?
	
	Shouldn't be ews isso related. Should be flag related
	
	-------------------------

2 cases for Default values:						
							PreferenceName passed: 
								entityList empty      :	getFromPreferenceNamesPassed
								entityList with values: if equal continues automatically ELSE minus
							
							PreferenceName not passed: 
									entityList empty      : getAllFromMap
									entityList with values: continues automatically.
							
							
----------------------------------------------------------------
Repair Fields for LINKAGE errors on Error Corrections pages:

As of 06/10/2020, CATCO updates err_rec_info table in catco_owner schema with the latest data for Ingestion Errors after it has been repaired. However, for Linkage Errors, the data in err_rec_info table never gets updated by CATCO after the repair. Thus, to be consistent and to have the up-to-date data we need a way to retrieve latest data for Linkage Errors.
Database design : Added New Columns in pndng_rec : sbmtd_by, sbmtr_org_id, upld_ts

Steps: (Data date: 05/20/2020, 05/21/2020)

1. Error Search Screen → Repair an error → Record created in pndng_rec table with repairedStatus = 'P'. 
2. Pending Events Screen → Submit the pending events → 
	1. /events/submissions   API is called and pndng_rec  table is updated.   repairedStatus →  'S', submittedBy → 'Logged-in-user-id' and submitterOrgId -> 'logged-in-user-org-id'
	2. A SQS message is sent → Lambda triggered
                             → Lambda calls   /events/submissions/files  API
                             → API generates and register files to DM.  pndng_rec  table is updated.  uploadTimestamp → 		
							   'upload-completion-timestamp'
                             → err_rec_info table is updated. Based on submissionId, the pending records are located and the repairType, 
							   repairedBy, repairedTimestamp, repairOrgId and repairSource in err_rec_info is updated

1. HardCode repairType to RPR if repairStatus is R, and repairSource to WEB in the API for Linker errors? ***
2. Distinguish Linkage(LINKAGE) and ingestion(REJECTED) error based on errorCode or errorType column? *** errorType is fine but we dont 
	need it not since we are updating the error record table directly, and errorRoeId will give exact records
3. Doing in java code creates N+1 problem, need to do using join in the query or populate the catco table with details? ***Design discussion
4. Columns to add to pending records table : uploadTimestamp and submitterOrgId and submittedBy? ***
5. Verify if updtd_by in pending records table is the logged in user ?	*** Yes it is
6. How does filter work on columns that populate from both error info and pending records table? Should we do it in the java code 
	just like error code counts api? *** We populate err_rec_cd table so this isn't an issue anymore
7. Update the name of different fields repair vs repaird. Verify and update?
8. add feature flag to the API so that if in the future CATCO decides to populate the LINKER details in err_rec_info table, we will disable
	the feature. We will however continue to populate the pending table. ***
9. repairOrgId is it orgId or firstName+lastName? *** its the orgId
10. Assume : Submitted 10 pending events which had 3 different IMIDs. 3 data files are created. Each data file is uploaded and registered 
	separately so each file will have different uploadTimestamp. we can use the file_sbmsn_info_id to find the record in pending table then 
	update the uploadTimestamp. Because if we use eventSubmissionId, all files will have same uploadTimestamp ??? *** Yep, just do batch update.
11. Support export for newly added column filters.	***
12. Populate filename column too in error rec info table.	***
13. Enhance query. Consider index ???	***
14. NPE check for errorRoeId And Check for new events.	***
15. groupRepairEligible ???
16. undo changes for renaming repaired -> repair ***

          
  if (rowsUpdatedErrorRecords == 0)
            {
                throw new ObjectNotFoundException(String.format("The error records for errorRoeId \"%s\" could not be updated.",
                    pendingEventInfoList.stream().map(PendingEventInfo::getErrorRoeId).collect(Collectors.joining(", "))));
            }

When repairing an error, if the data has bad value(example starting with invalid character), the 400 is not handled in UI. Nothing happens.
try correcting errorRoeId 4245713056757

Things to verify:
1. ErrorSearch API has added columns: rpr_type = 'RPR' and rpr_src = 'WEB'	***
2. ErrorSummary API has rpr_type = 'RPR'	
3. Export for both APIs. Sort and Filter on new Columns in error search	
4. API flow, Check Pending Update + Error Update. ***
5. Check if the Bulk update is done. ***
6. Check for LINKAGE + ingestion(REJECTION) error records. Ingestion records should not be updated	***
7. What about group repair and all? *** Nothing
8. Check the code coverage. *** 92%

.error(String.format("Unexpected error occurred. Only %d out of %d error records updated.", rowsUpdated, errorRoeIdsChunk.size()));
            LOGGER.info(String.format("Updating Error records for eventSubmissionId \"%d\" for Linkage errors", eventSubmissionId));


9. EventIdChunk check for 6, 4

---------------------------------------------------------
What happens In pending Submission API:
1. Get the next available event submission id.
	eventSubmissionDao.getNextAvailableEventSubmissionId()
2. Submit the pending event in pending table.
	eventSubmissionDao.submitEvents(...........) -> Based on filterModel and based on EventIds
3. Group the events based on IMID to create files for similar imid
	eventSubmissionDao.groupEventsOnReporterImids(eventSubmissionId);
4. Get submission Id for data file.
	eventSubmissionDao.getNextAvailableFileSubmissionId()
5. Get submission Id for meta file.
	eventSubmissionDao.getNextAvailableFileSubmissionId()
6. Create file submission for meta file.
	eventSubmissionDao.createFileSubmission(........)
7. Use create data file submission using associated meta file submissionId.
	eventSubmissionDao.createFileSubmission(........)
8. Update the selected events with the data file name.
	eventSubmissionDao.addEventsFileSubmissionId(......)
--------------------------------------------------------------
Update upload timestamp from upload start time to upload end time:

1. UploadTimestamp: Update the herd attribute before setting status to valid.	***
2. If upload error, set status to invalid.	***
3. Remove submitTimestamp request param. Update api spec.	***
4. Unit tests	***

Run automation in QA environment:
	HibernateUtil:			cfg.getProperties().setProperty("hibernate.connection.password", "DSF09ujlk9hlKK09upujm");
	DDRequest: 	String password;
				if (StringUtils.equals(user, "tst_catdd_d"))
				{
					password = "18!0}1N84Cp7P<%b{!\"yE5ry";
				}
				else if (StringUtils.equals(user, "tst_catdd_q"))
				{
					password = "xE2>$Dku48~l=n@4_oqSDI.A";
				}
				else
				{
					password = "portalimuser12314!";
				}
	DatabaseUtil:         String password = "DSF09ujlk9hlKK09upujm";
	catdd.properties : dev

--------------------------------
Ag-grid testing:

Not use ag-grid:
1. Reporting summary
2. event type counts
3. Create cat eventSourc
4. File upload

Test in firefox:
-----------------------------------

Ingestion -> Event Stats

Intrafirm -> Linkage 

File -> Ack, Integri


		{
	      "colId": "catReporterIndustryMemberId",
	      "sort": "asc"
	    },
	     {
	      "colId": "errorCode",
	      "sort": "asc"
	    }



	"catSubmitterId": {
			   "filterType": "text",
			   "type": "equals",
			   "filter": "90012"
		   },
   		"catReporterIndustryMemberId": {
			   "filterType": "text",
			   "type": "equals",
			   "filter": "JPMS"
		   },
		"errorCode": {
			   "filterType": "text",
			   "type": "equals",
			   "filter": "2150"
		   },
		"errorType": {
			   "filterType": "text",
			   "type": "equals",
			   "filter": "REJECTION"
		   },
	   	"date": {
		   "filterType": "date",
		   "type": "equals",
		   "filter": "2020-05-11"
	   }
	   
	   
	   
	   
	   
	   [
		{
	      "colId": "date",
	      "sort": "asc"
		},
		{
	      "colId": "catReporterIndustryMemberId",
	      "sort": "desc"
		},
		{
	      "colId": "catSubmitterId",
	      "sort": "desc"
		},
		{
	      "colId": "errorCode",
	      "sort": "desc"
		},
		{
	      "colId": "errorType",
	      "sort": "desc"
		},
		{
	      "colId": "errorCount",
	      "sort": "desc"
		}
	]
	
	
	
	{
		"statusCode": 400,
		"statusDescription": "Bad Request",
		"message": "The column \"Error Reason\" is not supported for sort.",
		"messageDetails": []
	}
	

	{
		"statusCode": 400,
		"statusDescription": "Bad Request",
		"message": "The column \"Error Reason\" is not supported for filter.; nested exception is java.lang.IllegalArgumentException: The column \"Error Reason\" is not supported for filter.",
		"messageDetails": [
			"The column \"Error Reason\" is not supported for filter."
		]
	}
	




------------------------------------------------------------------
05/20/2020
ErrorCodeCounts export:
1. Update the Api spec for errorCodeCounts *** Done
2. sort on date, update code?	*** Done
3. There is a weird character in the error reason ??? *** In an excel thing
	Invalid combination ofÂ aggregatedOrders and representativeInd	
4. Should perspective and productType header be like UI? Currently like other screens. **** Dont need to worry about it right now
5. ALL is less than Options + Equities combined.
------------------------------------------------------------------
05/13/2020
ErrorCodeCounts:
1. Filter on errorReason? *** No. But Sort Yes
2. Filter on ErrorCount ? *** Can't do because its not a physical column
3. Merge package catco and catdd service ??? *** Done
4. Ask himanshu how his caching for catmarket work? *** Done
5. ALready applied filter do we apply again for filtermodel or what? **** yes
6. Is filter value case-sensitive?	*** To be consistent with other api, yes. But ideally should be NO.
7. Why error message different for Enum and the Switch in ErrorCodeCountsDao for Column filters different? 					??????????????????
8. Submitter id and stuff is String in db, so should I have it as String or Integer in errroCodeCountsDetail. *** Entity can be String, but response dto
	needs to be cast to String.
9. Add pagination Support *** Done
10. Add javaDoc ***
11. Unprb in errorCodeLookup table *** Unparsable

Acceptance criteria:
1. Sort on All the columns except errorReason. Try each column. Sort with wrong column and one right column	***
2. Filter on All the columns except errorReason and errorCount. Try each column	***
3. Try sort and filter together with multiple columns. 	***
4. Default sort. 	***
5. Test default top level filters including new perspective THIRD_PARTY	***
6. Verify the dataType returned	***
7. Test pagination. *** 																				Update for errorSummary and all -ve values
8. Test with Missing required fields and stuff for correct exceptions	***
9. LastRow how to calculate ?	*** As of now, get the list and get the size()
10. ErrorCode sort should work. Update the error message too.	***
11. Check for dataTuple as null		***
12. Check other apis after catco and catdd service packages merged.
13. For Post call with exact same request with date sort asc after 6th call gives result similar to default sort? **** When sort column has same value,
	order can be random
14. Provide the filter Type in api spec.	***
15. If error reason not found, set as null ? and log exception ****


Test:
List size =0
(startRow, endRow):		(0, -1), (0, 0), (0, 1), (0, 2), (1, -1), (1, 0), (1, 1), (1, 2), (2, -1), (2, 0), (2, 1), (2, 2)
ListSize = 2
(startRow, endRow):		(0, -1), (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, -1), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (2, -1), (2, 0), (2, 1), (2, 2),  (2, 3), (2, 4),
The issue is if the size of list is 10, and if the startRow is after 10, the code is erroring out because the subList index has issues now.
---------------------------------------------------
Random knowledge:
Event Stats -> Ingestion

Intrafirm -> Linkage 

File stats -> Ack, Integrity
-------------------------------------------------
Export: Include inline filter and sort
1. Verify the Applied Inline Filters and Sorted by does not show if nothing is there.	***
2. Verify for all the filters and sort.
3. Comma delimiter issue in excel.
4. Test sort and filter seperately.	***
--------------------------------------------------------
ThirdParty perspective in error correction screen api :
Dates for data in db, 04/20/2020, 04/21/2020
APIs affected: 8 
1) IMID:	/api/v1/industryMemberIds?
	1. IMID list is same as submitter. Submitter, ThirdParty and ALL have same results
2) Error Summary :	 /api/v1/events/errors/summary
	1. When thirdParty perspective selected, fetch data.	
	2. IMID dropdown list for thirdParty perspective	
	3. Make sure filter/sort still works	
	4. hyperlinks ? GroupRepair, ErrorCount -> UI part	
	5. When ALL perspective, include ThirdParty

 When Y is selected in groupRepairEligible column in Error Summary ->
3) Get the group repair eligible error field information : /api/v1/errorFields/groupRepairEligible/2082
4) Group Repair:	/api/v1/events/errors/groupRepairEligible
	1. Navigate to Group repair when selected Y from ErrorSummary UI.
	
 When ErrorCount is selected in Error Summary	->
5) Error Search :	/api/v1/events/errors
	1. When thirdParty perspective selected, fetch data.	
	2. IMID dropdown list for thirdParty perspective	
	3. Make sure filter/sort still works	
	4. hyperlinks ? UI part		
	5. Make sure Export works	
	6. When ALL perspective, include ThirdParty
	
 When errorRoeId is selected from Error search  ->
6) Retrieve Error Event based on errorRoeId :	/api/v1/events/errors/2000007000000010000?
	1. Make sure thirdParty is supported 
7) Retrieve Event type details : /api/v1/eventTypes/MONO
8) ErrorRoe Repair:	/api/v1/events
	1. Navigate to Error Repair when errorRoeId is selected from errorSearch UI.
	2. Make sure thirdParty is supported.
	
Questions:
1. What about pending submission because right after the error correction is done, it gives a link to that page ? *** UI will disable the pop up.
2.Not understanding the query ????
	SELECT ERC.err_roe_id errorRoeId, ERC.ERR_CD errorCode, ECL.ERR_DS errorReason, EFL.ERR_FLD errorField FROM catco_owner.ERR_REC_CD ERC,
	catdd_owner.ERR_CD_LK ECL, catdd_owner.err_fld_lk EFL WHERE ERC.err_roe_id='5000000000000006580' AND ERC.ERR_CD=ECL.ERR_CD AND EFL.ERR_CD=ERC.ERR_CD
3. Error Repair 400 not handled in UI. for ex: prohibited character ?
4. Filter: 	1. for date UI does not make a call with that, so why show filter option?  And if not showing for filter, why call for sort?
			2. for groupRepair N is not present but we can filter for N ?
			3. Filtering on errorReason for MULTIPLE ERRORS will not work ?
5. Why does buildWhereClause() in EventDaoImpl have <if (catSubmitterId != null)> apart from perspective ??? for submitter perspective, it will be
	twice?
6. Explain james why I have put some Perspectives as Thirdparty replacing Submitter because there are multiples
7. For error roe repair, even though the error only shows 2, there is the eventTimestamp that is mandatory.
8. when is the endpoint /events PUT used? because for error roe repair we use POST
9. IMID : is thirdParty same as submitter and all?..............osorelationship


1. Submitter perspective -> Logged in 79. When error is repaired, SubmitterId=79, and using it with the IMID from request, we get ReporterId from OSO.
2. Reporter perspective -> logged in 79. When error is repaired, SubmitterId=79, and ReporterId=79
3. Third party perspective -> logged in 7059. When error is repaired, SubmitterId=79, and using it with the IMID from request, we get ReporterId from OSO. We treat ThirdParty just like submitter, because technically submitter is the creator of the correction record, and regardless of thirdParty or
submitter perspective we still look up OSO to check the relationship for the logged in firm to populate ReporterId.
NOTE: ThirdParty column in db remains empty. 

Data: TOMCAT2 imid do not have OSO relationship for both crd 36187 and 12030. Also the relationship should be for submitter 7059. Need to update
		Fetch the data by doing where crtd_by='K28172'
-----------------------------------------------------------------------
Automation: Plan participant family relationships
1. What entitlements in Finra cat can access the api? ****** admin doesn't. rest does
2. Create User accounts with required entitlements ? *****
------------------------------------------------------------------------
application.yml in Himanshu's PR:

Already existing:
	catprivcatddadmin				- ALL
	catprivcatdduser				- ALL	

Newly added:
	catPrivCatddPPUser				- ONLY MATCHING FAMILY
	catPrivCatddPPComplianceUser	- ONLY MATCHING FAMILY
	catPrivCatddSECComplianceUser	- ALL
	catPrivCatddComplianceUser		- ALL

------------
Actual :

catPrivCatddPPUser 				- ONLY MATCHING FAMILY
catPrivCatddPPComplianceUser 	- ONLY MATCHING FAMILY
catPrivCatddUser 				- ALL
catPrivCatddComplianceUser 		- ALL
catPrivCatddSECComplianceUser	- ALL

------------
https://jira.finra.org/browse/CATDD-2137
Story : CATDD-2137

Plan Participant Reporter User	: catPrivCatddPPUser
Plan Processor User+			: catPrivCatddUser
Plan Processor Compliance User+	: catPrivCatddComplianceUser
SEC Compliance User				: catPrivCatddSECComplianceUser
-------------------------------------------------------------------------
1. For Csv injection, made the fix in feature/CATDD-311. Might need to pull the changes and merge it
-------------------------------------------------------------------------
Group repair eligible query update:
1.
	{
		"statusCode": 500,
		"statusDescription": "Internal Server Error",
		"message": "PreparedStatementCallback; SQL [SELECT date, abc.cat_rprtr_imid catImid, abc.cat_sbmtr_id catSubmitterId, abc.rprd_smry repairedStatus, abc.grp_rpr_fl groupRepairFlag, abc.err_type errorType, abc.err_cd errorCode, l2.err_ds errorReason, abc.rpr_type_cd repairedType, COUNT(*) errorCount FROM (SELECT eri.prcsg_dt date, eri.err_roe_id err_roe_id, eri.cat_rprtr_org_id cat_rprtr_org_id, eri.cat_rprtr_imid cat_rprtr_imid, eri.cat_sbmtr_id cat_sbmtr_id, eri.prdct_type prdct_type, case when eri.rprd_st = 'R' then 'R' when eri.rprd_st = 'U' and pnd.err_roe_id is null then 'U' else pnd.rprd_st end as rprd_smry, case when eri.rprd_st = 'R' then 'N' when eri.rprd_st = 'U' WHEN eri.rprd_st = 'U'  AND pnd.err_roe_id IS NULL AND eri.grp_rpr_elgbl = 'Y' THEN 'Y' else 'N'  end  as grp_rpr_fl, eri.err_type err_type, eri.rpr_type_cd rpr_type_cd, trim (unnest(regexp_split_to_array(eri.err_list, ',')))::int err_cd FROM catco_owner.err_rec_info eri LEFT JOIN (select err_roe_id, max(rprd_st) as rprd_st from catdd_owner.pndng_rec where err_roe_id is not null group by err_roe_id) pnd ON ( eri.err_roe_id = pnd.err_roe_id )  (eri.cat_rprtr_org_id=? OR eri.cat_sbmtr_id=?) AND eri.prcsg_dt = ? ) ABC JOIN catdd_owner.err_cd_lk L2 ON (L2.err_cd = ABC.err_cd)  GROUP BY date, abc.cat_rprtr_imid, abc.cat_sbmtr_id, abc.rprd_smry, abc.grp_rpr_fl, abc.err_type, abc.err_cd, L2.err_ds, abc.rpr_type_cd   ORDER BY date, abc.cat_rprtr_imid, abc.cat_sbmtr_id, abc.rprd_smry, abc.grp_rpr_fl, abc.err_type, abc.err_cd LIMIT 21 OFFSET 0]; This connection has been closed.; nested exception is org.postgresql.util.PSQLException: This connection has been closed.",
		"messageDetails": [
			"This connection has been closed."
		]
	}
	Should the Sql exception be thrown?	
2. For Error records date, for testing purpose: 2019-02-14 ***
3. For errorSummary, errorCode is of type "number", whereas for errorSearch it is of type "text" while using filterModel.
4. For a record to be group repair eligible:
	1. Should have only one error code(not multiple)
	2. Should be group repair eligible errorCode(err_cd_lk table)
	3. RepairedType is Unrepaired(U)
-----------------------------------------------------
Automation: Error search export : 
-> 2008-08-27 = Half million records; 2010-08-27 = 100K records, 2016-08-27 = Some records
0. Why Integer? for symbol *** Updated
1. What if folder already has some file?	***Taken care of
2. If multiple error codes, should we send null as request for error code or comma separated?
3. shouldn't export and error search have same data? ***Yes
	{
	  "startRow": 1,
	  "endRow": 20,
	   "filterModel": {
		"errorCode": {
			   "filterType": "text",
			   "type": "equals",
			   "filter": "2018"
		   }
	  },
	  "sortModel": [
	   ],
	  "errorSearch" : {
			"date": "2019-12-10",
			"dateType": null,
			"productType": null,
			"catOrganizationId": 79,
			"catSubmitterId": null,
			"catReporterIndustryMemberId": null,
			"perspective": "ALL",
			"errorCode": null,
			"groupRepairEligible": null,
			"repairedStatus": null,
			"errorType": null
	  }
	}
4. Filters hasnt been done in error search.....???? Doesn't look like
5. Sort 2nd column.
6. Audit information date format ? *** Fixed
7. String firmRoeId = String.format("XOXO_FirmROE_%d", i);
	String orderId = String.format("XOXO_OrderID_%d", i);

	{
	  "queryList": [
		{
		  "name": "insertErrorRecordInfo",
		  "value": "INSERT INTO catco_owner.err_rec_info(err_roe_id,file_unq_id, cat_rprtr_org_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, file_nm, file_frmt, prcsg_dt, trd_dt, prdct_type, rprd_st, err_type, actn_type,  firm_roe_id, err_list, msg_type, crtd_ts, crtd_by, updtd_ts, updtd_by)  VALUES(:errorRoeId, 10, :orgId, :catReporterImid,:orgId, null, '79_X0X0_20200101_CatWeb_OrderEvents_000001.json', 'TXT','2010-08-27', '2010-08-27', :productType, :repairedStatus, :errorType, 'RPR', :firmRoeId, :errorCodes, 'MONO', current_timestamp, 'k28172@nasdcorp', current_timestamp, 'k28172@nasdcorp')"
		},
		{
		  "name": "insertErrorRecordDetail",
		  "value": "INSERT INTO catco_owner.ERR_REC_DTL(actn_type, err_roe_id, firm_roe_id, msg_type, cat_rprtr_imid, odr_key_dt, odr_id, optn_id, event_ts, mnl_fl, mnl_odr_key_dt, mnl_odr_id, elctc_dplct_fl, elctc_ts, dept_type, side, pr, qty, min_qty, odr_type, tif, trdg_sssn, hndlg_instr, firm_dsgnt_id, accnt_hldr_type, afflt_fl, agrtd_odrs, opn_cls_ind, rep_ind, next_unlkd, crtd_ts, crtd_by, updtd_ts, updtd_by) VALUES('RPR', :errorRoeId, :firmRoeId, 'MONO', :catReporterImid, '20191203T123456.000501', :orderId, 'XTEC', '20200101T123456.000501', 'false', null, null, 'false', null, 'DMA', 'SS', '111.00', '200', '50', 'LMT', 'DAY=20191208', 'POST', 'DISQ=1234|STOP=90.91', 'Firm-DID-102', 'F', 'true', 'ORD101@20191204T101534.000101@209@|ORD101@20191203T101234.000102@202@', null, 'Y', null, current_timestamp, 'k28172@nasdcorp', current_timestamp, 'k28172@nasdcorp')"
		},
		{
		  "name": "insertErrorRecordCode",
		  "value": "insert into catco_owner.ERR_REC_CD(err_roe_id, err_cd, crtd_ts, crtd_by, updtd_ts, updtd_by) values(:errorRoeId, :errorCode, current_timestamp, 'k28172@nasdcorp', current_timestamp, 'k28172@nasdcorp');"
		},
		{
		  "name": "insertPendingRecord",
		  "value": "INSERT INTO catdd_owner.PNDNG_REC(pndng_rec_id, cat_rprtr_org_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, actn_type, err_roe_id, firm_roe_id, msg_type, odr_key_dt, odr_id, optn_id, event_ts, mnl_fl, mnl_odr_key_dt, mnl_odr_id, elctc_dplct_fl, elctc_ts, dept_type, side, pr, qty, min_qty, odr_type, tif, trdg_sssn, hndlg_instr, firm_dsgnt_id, accnt_hldr_type, afflt_fl, agrtd_odrs, opn_cls_ind, rep_ind, next_unlkd, crtd_ts, crtd_by, updtd_ts, updtd_by, rprd_st) VALUES(nextval('pndng_rec_seq'), :orgId, :catReporterImid, :orgId, null, 'RPR', :errorRoeId, :firmRoeId, 'MONO', '20191203T123456.000501', :orderId, 'VIRT', '20200101T123456.000501', 'false', null, null, 'false', null, 'DMA', 'SS', '111.00', '200', '50', 'LMT', 'DAY=20191208', 'POST', 'DISQ=1234|STOP=90.91', 'Firm-DID-102', 'F', 'true', 'ORD101@20191204T101534.000101@209@|ORD101@20191203T101234.000102@202@', null, 'Y', null, current_timestamp, 'k28172@nasdcorp', current_timestamp, 'k28172@nasdcorp', 'P')"
		}
	  ]
	}
8. Jvm out of memory ????	*** Because of CsvToBean parse. Used basic java for loop and CsvReader to fix it. 
	00:52:44.425 [pool-2-thread-8] DEBUG o.a.c.b.converters.DoubleConverter - Setting default value: 0
	00:52:44.426 [pool-2-thread-8] DEBUG o.a.c.b.converters.DoubleConverter - Converting 'Integer' value '0' to type 'Double'
	00:52:44.427 [pool-2-thread-8] DEBUG o.a.c.b.converters.DoubleConverter -     Converted to Double value '0.0'
	00:52:44.426 [pool-2-thread-7] DEBUG o.a.c.b.converters.ArrayConverter - Setting default value: [Ljava.math.BigDecimal;@9b2d51d
	00:52:44.427 [pool-2-thread-7] DEBUG o.a.c.b.converters.ArrayConverter - Converting 'BigDecimal[]' value '[Ljava.math.BigDecimal;@9b2d51d' to type 'BigDecimal[]'
	00:52:44.427 [pool-2-thread-7] DEBUG o.a.c.b.converters.ArrayConverter -     No conversion required, value is already a BigDecimal[]
	00:52:44.426 [pool-2-thread-1] DEBUG o.a.c.b.converters.ArrayConverter - Converting 'Short[]' value '[Ljava.lang.Short;@25ebbf34' to type 'Short[]'
	00:52:44.427 [pool-2-thread-1] DEBUG o.a.c.b.converters.ArrayConverter -     No conversion required, value is already a Short[]
------------------------------------------
Customer reporting relationships API GET:
1. Who can have access ??? *** for now just let read-only 
2. order by ??? *** updt_ts desc, crtd_ts desc
3. if all the query params are optional, why is the check for orgIdpermission still active. For ATS and OSO also??? *** Dont need orgIdPermission for this
	API. For OSO and ATS, ISSO user can access without any params while ews users have to have atleast reporter, submitter or thirdparty id.
4. Provided effective date means records effective on or before that date ??? 
5. update tradekeyDate name ???? **** Done
6. EventSubmissionGroupKey, AtsOrderTypeKey class never used ????? can I remove? *** YES
7. .matches() not working for texts with multi line ????

Enumeration...... User-agent
------------------------------------------------------------------
Announcements:
Post:
1. Himanshu's CSV injection tests gives errors in random order for every post call. *** 
2. Expiration date cannot be a past date???	****
3. created ts not eastern *** Db has time zone
4. Timestamp .0 extra **** Removed code
5. Rich-text characters	*** Removed restriction from body, only title size check added
6. Default false for urgent ***
7. Expiration date wrong error message. Use expirationTimestamp instead of expirationDate. ??? Enhancement ***
8. Csv injection ??? Enhancement ***
9. Trim the input ??? Enhancement ***
10. Remove the getter for entity class ???	***
11. AnnouncementRequest getting npe ??? ***
12. himanshu csv injection error message update ???	***
13. trim happens after validation. So "    +apple" passes validation and is saved as "+apple" in db ????? **** created a custom annotation

1. Internal vs external users. Security model.
2. figure a way to sort*** found

INSERT INTO catdd_owner.ancmt(
	ancmt_id, title, ancmt_ctgry_id, msg_body, ancmt_stts_id, pblsh_ts, pblsh_by, aprvl_ts, xprtn_ts, crtd_ts, crtd_by, updtd_ts, updtd_by)
	VALUES (NEXTVAL('catdd_owner.ancmt_seq'), 'Exchange Route Match for February 25,2020',1, 'Exchange Route Match for February 25,2020 are now available except for CBOE EDGA Exchange and CBOE BZX Exchange.<br>Exchange Route Match for February 25,2020 are now available except for CBOE EDGA Exchange and CBOE BZX Exchange.Exchange Route Match for February 25,2020 are now available except for CBOE EDGA Exchange and CBOE BZX Exchange.Exchange Route Match for February 25,2020 are now available except for CBOE EDGA Exchange and CBOE BZX Exchange.',
			2, '2020-03-03 17:44:26.002023', 'DevTeam2', '2020-03-02 17:44:26.002023', '2022-03-02 17:44:26.002023+00', '2020-03-01 17:44:26.002023', 'DevTeam1', '2020-03-01 17:44:26.002023', 'DevTeam1');
			
if (!StringUtils.isEmpty(category))
{
	announcementCategoryEnum = AnnouncementCategoryEnum.find(category);
	announcementCategoryEntity = announcementHelper.getAnnouncementCategoryEntity(announcementCategoryEnum);
	categoryId = announcementCategoryEntity.getCategoryId();
	announcementEntityPage = announcementRepository.findAllByCategoryId(categoryId, PageRequest.of(pageNumber, pageSize));
	announcementEntityList = announcementEntityPage.stream().collect(Collectors.toList());
}
else
{
	announcementEntityPage = announcementRepository.findAllFinalAnnouncements(PageRequest.of(pageNumber, pageSize));
	announcementEntityList = announcementEntityPage.stream().collect(Collectors.toList());
}
		AnnouncementCategoryEnum announcementCategoryEnum = AnnouncementCategoryEnum.find(category);
		AnnouncementCategoryEntity announcementCategoryEntity = announcementHelper.getAnnouncementCategoryEntity(announcementCategoryEnum);
		Integer categoryId = announcementCategoryEntity.getCategoryId();
	Page<AnnouncementEntity> announcementEntityPage = announcementDao.findAllByCategoryId(categoryId, PageRequest.of(pageNumber, pageSize));
	List<AnnouncementEntity> announcementEntityList = announcementEntityPage.stream().collect(Collectors.toList());
	
/**
 * Repository interface for Announcements
 */
@Repository
public interface AnnouncementRepository extends JpaRepository<AnnouncementEntity, Integer>
{
    @Query(value = "SELECT ancmt FROM AnnouncementEntity AS ancmt INNER JOIN ancmt.announcementCategory category " +
        "INNER JOIN ancmt.announcementStatus status WHERE category.categoryId =?1  AND status.statusName = 'Final' order by ancmt.publishedTimestamp desc")
    Page<AnnouncementEntity> findAllByCategoryId(Integer categoryId, Pageable pageable);

    @Query(value = "SELECT ancmt FROM AnnouncementEntity AS ancmt INNER JOIN ancmt.announcementStatus status WHERE status.statusName = 'Final' order by" +
        " ancmt.publishedTimestamp desc")
    Page<AnnouncementEntity> findAllFinalAnnouncements(Pageable pageable);
}

    /**
     * Retrieve the announcement category entity based on announcement category
     *
     * @param announcementCategoryEnum the announcement category enum {@link AnnouncementCategoryEnum}
     *
     * @return the announcement category entity
     */
    public AnnouncementCategoryEntity getAnnouncementCategoryEntity(AnnouncementCategoryEnum announcementCategoryEnum)
    {
        return announcementCategoryDao.findByCategoryName(announcementCategoryEnum.getValue()).orElseThrow(() -> new CatddRuntimeException(
            String.format("Cannot find the announcement category '%s' in announcement category lookup table.", announcementCategoryEnum.getValue())));
    }
------------------------------------------------------------------------------------------------
1. How is it registered in herd? without actually doing anything. Just Creating an object and then what ????
2. How did he initially know about it? Jams call to lambda handler? Can't find in google
3. TimeTracker
------------------------------------------------------------------------------
Submission Id, from s3 location.....or from staging location?.........

1. 1000 files takes a long time. Sometimes the file arenot available in s3. Maybe the debug logger needs to be disabled.
2. Submit a pending event, get the eventSubmissionId instead from querying from db. ***
3. Rename packages lowercase	***
4. Null pointer in get submission id ? could not reproduce
5. Do we Need to check for different perspective? *****
6. why some fields null for mismatch *****Tied to number 8
7. Error message match mismatch ? Dont know the order. ******Do a manual verification
8. In pendingSubmissionTable, after submitting....(Saravanah will look into)
	-> ALL OK
	orderEventsList
           .add(Arrays.asList("MECO", "NEW", "", "2019_SM_firm_3", "ZBTS", "20200203T123456.000501", "order_9999_New", "SYM", "20200204T123456.000501"));
        
	-> parentOdrKeyDate and symbol missing. WHY?
	orderEventsList
            .add(Arrays.asList("MONO", "COR", "", "2019_SM_firm_2", "JPMS", "20200203T123456.000505", "order_1111_New", "SYM", "20200204T123456.000505"));
    
	-> odr_key_dt, odr_id, parentOdrKeyDate and symbol missing. WHY?
	 orderEventsList
            .add(Arrays.asList("MOFA", "RPR", "", "2019_SM_firm_3", "JPMX", "20200203T123456.000508", "order_2222_New", "SYM", "20200204T123456.000508"));
9. If folder not exist, then create folder. ****



actualFileSubmissionInfoRecords = fileSubmissionInfoDao.getByEventSubmissionId(new Long(eventSubmissionId));
List<String> generatedFileNames = actualFileSubmissionInfoRecords.stream().map(o -> o.getFileName()).collect(Collectors.toList());

String folderName = STAGE_BASE_PATH + "/" + eventSubmissionId;
List<String> s3FileNames = getS3FileNames(STAGE_BUCKET, folderName);

runOn=Local
user=k28172
pass=AlvaOklahoma0827!

---------------------------------------------
For loop time comparision for s3 file :

Stream : parallelStream().forEach()   -> 3 min 20 sec download 1006 files
Stream : Stream().forEach()   -> In 3 min 20 seconds only downloaded 106 files


--------------------------------------------
Comprehensive stats:
testGetStatisticsWithProductTypeEmptyStr.
1. API needs to add ACPTD_RJCTD_IND = 'C' for sum(RPRD_RCRDS) calculation **** Later. Bug created
{
    "date": "2020-02-15",
    "dateType": "ProcessingDate",
    "organizationId": 79,
    "perspective": "ALL",
    "productType": "ALL",
    "total": {
        "files": {
            "validFiles": 0,
            "invalidFiles": 0,
            "dataFilesReceived": 0,
            "dataFilesAccepted": 0,
            "dataFilesRejected": 0,
            "metadataFilesReceived": 0,
            "metadataFilesRejected": 0
        },
        "events": null
    },
    "list": []
}
2. why is events null while files has it all as 0 ?????
3. Why not found meesage for oso and ats for 90012 user	*** Bug created
4. APi response missing the row with null imid that contains the total number of invalid files. **** Bug created. May or may not be done. BA okay with anything

------------------------------------------------
API automation for event type counts:
1. Tuples ? what are the indexes? what column does it refer to? -> In query whatever column comes first is the first index in tuple
2. wiki page update productType case sensitive, and default
3. Query for processedRejected, processedAccepted and processedDeleted
4. different productType
5. In statistics, trade date logic is wrong.	*** I will filx it
6. If dateType is TradeDate and date is null. Then it still returns processing date. *** Himanshu made the fix
7. testEventTypeCountForRecordsWhereActionTypeIsNull, reporter or submitter perspective how to choose? random?
8. Connection created multiple times?? **** Passed for each Test
9. FIle Status service has unused imports and unused stuffs. no violations??? **** its in test class so 



check for method where :
    @Test
	public void nothing()
	{
		List<String> list = new ArrayList<>();
		String text = null;
		list.add(text);
		text = "still works";
		String check = list.get(0);
	}
	Check stas api automation
-----------------------------------------------------------------
API automation for fileStatus:
1. why not equals for error message comparision. **** because we are appending some other message to what we got.
2. equals actual vs expected. ***
3. test data for 79  ***
4. Should be Sorted by upload timestamp . And what if they are same ???? Currently I am doing by name next **** Done. Differently
5. For latest ProcessingDate, the results for 79 is usually empty list **** Check added
6. Remove env=dev from check in ****
7. Update submitterId to uploadOrgId -> Comprehensive stats too.   ****
8. rename latestProcessingDate to latestProcessingDateWithData for most of them ***

-----------------------------------------------------------------

    private String getLatestTradeDateWithData(String catOrganizationId, Perspective perspective, String dateType, String catReporterIndustryMemberId,
        String productType)
    {
        List<String> tradeDates = getLatestTradeDatesList();
        String latestTradeDate = getLatestTradeDate();

        List<EventTypeCountsDaoVO> dbResultInitial = EventTypeCountsHelper
            .getEventTypeCountsDaoVoList(catOrganizationId, perspective, latestTradeDate, dateType, catReporterIndustryMemberId, productType);

        int index = tradeDates.indexOf(latestTradeDate);
        while (dbResultInitial.isEmpty() & index > 0)
        {
            index = index - 1;
            latestTradeDate = tradeDates.get(index);
            dbResultInitial = EventTypeCountsHelper.getEventTypeCountsDaoVoList(catOrganizationId, perspective, latestTradeDate, null, null, null);
        }
        return latestTradeDate;
    }

-------------------------------------------------------------------------------------------------------
FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName= , pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 20:13:05, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 20:13:15, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.meta.json, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:49:25, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:32, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.meta.csv, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:49:15, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:22, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.json.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:49:05, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:12, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.csv.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:48:55, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:03, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=_____.csv.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:40:08, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:40:17, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79_____.csv.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:37:37, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:37:45, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79_.meta.json, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:34:11, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:34:19, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79_.meta.csv, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:34:01, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:34:09, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79_.json.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:33:51, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:33:59, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79_.csv.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:33:39, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:33:46, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79.meta.json, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 16:52:57, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 16:53:03, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79.meta.csv, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 16:52:47, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 16:52:54, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79.csv.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 16:52:34, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 16:52:40, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))

FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79.json.bz2, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 16:52:18, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 16:52:25, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))



FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.meta.json, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:49:25, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:32, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=null), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))
FileStatusDetailOutput(catSubmitterId=79, catReporterIndustryMemberId=null, userId=k28172@nasd.corp, uploadedFileName=79__20170101_Pokemon_OrderEvents_000123.meta.json, pairedMetaDataFileName=null, upload=FileStatusUploadDetailOutput(timestamp=2020-02-05 19:49:25, method=WEB), acknowledgement=FileStatusTimeStampInfoOutput(timestamp=2020-02-05 19:49:32, fileStatus=Failure), integrity=FileStatusTimeStampInfoOutput(timestamp=null, fileStatus=Duplicate), ingestion=FileStatusIngestionDetailOutput(timestamp=null, processed=0, accepted=0, rejected=0))
 

CASE WHEN D.ACKNOWLEDGE_RESULT = 'Success' 
 This is false for above records which is why it does not change to Timeout or Duplicate

-------------------------------------------------------------------------------------------------------

REPORTER:
	1. Get memberDictionaryList for last 90 days
	2. Get IMIDs from that

SUBMITTER:
	1.  Get memberDictionaryList for last 90 days
	2.  Get OSO realationship for last 90 days for submitterId=LoggedInUser
	3. For every relationship, and memberDictionaryList, if(imidFromOso=ImidFromMember AND reporterOrgForOso=CrdFromMember ) -> add to imids


Relationship gives Reporters whose IMID we need to lookup in memberDictionary table.....

SELECT distinct imid FROM catdd_owner.mstr_mbr_dict_hs where firm_crd_nb=79
Union 
Select distinct cat_rprtr_imid from catdd_owner.oso_rltnp_hs where cat_sbmtr_id=79 AND actv_fl=true
order by imid asc;


------------------------------------------------------------------------------------------------------------
72.7

//    @Test
//    public void testBuildWhereClauseForDerivedColumns_NotNullRepairedStatus_NotNullGroupRepairEligibility()
//    {
//        assertEquals(" WHERE  info.rprd_smry = :repairedStatus2 AND  grp_rpr_fl = :groupRepairEligible",
//            eventDao.buildWhereClauseForDerivedColumns(RepairedStatusEnum.PENDING, true));
//    }

    @Test
    public void testBuildWhereClauseForDerivedColumns_NullRepairedStatus_NotNullGroupRepairEligibility()
    {
        assertEquals(" WHERE  grp_rpr_fl = :groupRepairEligible", eventDao.buildWhereClauseForDerivedColumns(null, true));
    }

    @Test
    public void testGetTotalCountErrorSummaryDetails()
    {
        Long mock = mock(Long.class);
        ArgumentCaptor<String> queryCaptor = ArgumentCaptor.forClass(String.class);
        ArgumentCaptor<Map<String, Object>> parameterMapCaptor = ArgumentCaptor.forClass(Map.class);
        when(eventQueries.getQueryValues(anyString())).thenReturn("%s|%s|%s|%s");
        when(namedParameterJdbcTemplate.queryForObject(anyString(), anyMap(), mock)).thenReturn(1200L);
        Long totalCount = errorSummaryDao
            .getTotalCountErrorSummaryDetails(ORG_ID, CAT_REPORTER_IMID, DateTypeEnum.PROCESSING_DATE, DATE, ProductTypeEnum.ALL, PerspectiveEnum.SUBMITTER,
                COLUMN_NAME_ENUM_LIST, FILTER_TYPE_ENUM_LIST, COMPARE_TYPE_ENUM_LIST, FILTER_VALUE_LIST);
        assertEquals(Long.valueOf(1200), totalCount);
        assertEquals(QUERY_GET_TOTAL_COUNT, queryCaptor.getValue());
//        verify(namedParameterJdbcTemplate).query(anyString(), anyMap(), Long.class);
    }
-----------------------------------
PendingEvents:																									86.7

 "sortModel": [
    {
      "colId": "catReporterIndustryMemberId",
      "sort": "desc"
    },
     {
      "colId": "errorCount",
      "sort": "asc"
    }
   ],
   
   
   "type": {
           "filterType": "text",
           "type": "equals",
           "filter": "NEW"
       },
        "errorRoeId": {
           "filterType": "number",
           "type": "equals",
           "filter": 10004
       },
        "quoteId": {
           "filterType": "text",
           "type": "equals",
           "filter": "54"
       },
        "orderId": {
           "filterType": "text",
           "type": "equals",
           "filter": "OrderID_4"
       }
-----------------------------------
ErrorSearch:
    
	"catReporterIndustryMemberId": {
           "filterType": "text",
           "type": "equals",
           "filter": "D"
       },
          "errorRoeId": {
           "filterType": "number",
           "type": "equals",
           "filter": 500004
       },
          "firmRoeId": {
           "filterType": "text",
           "type": "equals",
           "filter": "60004"
       },
          "errorCode": {
           "filterType": "text",
           "type": "equals",
           "filter": "2041"
       },
          "errorReason": {
           "filterType": "text",
           "type": "equals",
           "filter": "Missing or Invalid manualFlag"
       },
          "errorType": {
           "filterType": "text",
           "type": "equals",
           "filter": "Rejection"
       },
          "repairedStatus": {
           "filterType": "text",
           "type": "equals",
           "filter": "R"
       },
          "quoteKeyDate": {
           "filterType": "text",
           "type": "equals",
           "filter": "2020-05-04"
       }
	   
  },
    "sortModel": [
    {
      "colId": "quoteKeyDate",
      "sort": "desc"
    },
     {
      "colId": "errorType",
      "sort": "asc"
    }
   ],
----------------------------------------------------------------------------------------
Things need to do:(2020-01-13)
Unit tests 
COlumn sort filter,,multiple ***
Add security
Validations 
ERROR SUMMARY:

  "filterModel": {
      "errorCount": {
           "filterType": "number",
           "type": "equals",
           "filter": 1
       },
      "catReporterIndustryMemberId": {
           "filterType": "text",
           "type": "equals",
           "filter": "A"
       },
      "date": {
           "filterType": "date",
           "type": "equals",
           "filter": "2019-12-10"
       },
  
      "errorType": {
           "filterType": "text",
           "type": "equals",
           "filter": "Rejection"
       },
      "repairedStatus": {
           "filterType": "text",
           "type": "equals",
           "filter": "U"
       },
	  "catSubmitterId": {
           "filterType": "number",
           "type": "equals",
           "filter": 13
       },
	  "groupRepairEligible": {
           "filterType": "text",
           "type": "equals",
           "filter": "Y"
       },
	  "errorReason": {
           "filterType": "text",
           "type": "equals",
           "filter": "Missing or Invalid fulfillmentLinkType"
       },
	  "errorCode": {
           "filterType": "number",
           "type": "equals",
           "filter": 2041
       }
	   
  },
    "sortModel": [
    {
      "colId": "errorType",
      "sort": "desc"
    },
     {
      "colId": "catSubmitterId",
      "sort": "asc"
    }
   ],
  
  -------------------------------------------------------------------------- 
           StringBuilder whereClauseOuter = new StringBuilder(256);
        StringBuilder whereClauseOuterConditions = new StringBuilder(256);
        String havingClause = "";
        if (CollectionUtils.isNotEmpty(columnNameEnumFilterList) && CollectionUtils.isNotEmpty(filterTypeEnumList) && CollectionUtils.isNotEmpty(compareTypeEnumList)
            && CollectionUtils.isNotEmpty(filterValueList))
        {
            for(int i=0; i<columnNameEnumFilterList.size(); i++)
            {
                if (StringUtils.equals(columnNameEnumFilterList.get(i).getValue(), ColumnNameEnum.ERROR_COUNT.getValue()))
                {
                    havingClause = buildHavingClause(columnNameEnumFilterList.get(i));
                }
                else
                {
                    whereClauseOuterConditions.append(buildWhereClauseOuter(columnNameEnumFilterList.get(i)));
                    if(i < (columnNameEnumFilterList.size()-1) && !Objects.equals(columnNameEnumFilterList.get(i).getValue(), ColumnNameEnum.ERROR_COUNT))
                    {
                        whereClauseOuterConditions.append(" AND");
                    }
                    if(i == (columnNameEnumFilterList.size()-1))
                    {
                        whereClauseOuter.append(" WHERE").append(whereClauseOuterConditions);
                    }
                }
            }
        }
		
		    protected Pair<String, String> buildWhereAndHavingClauseOuter(List<ColumnNameEnum> columnNameEnumFilterList, List<FilterTypeEnum> filterTypeEnumList,
        List<CompareTypeEnum> compareTypeEnumList, List<Object> filterValueList)
    {
        StringBuilder whereClauseOuter = new StringBuilder(256);
        StringBuilder whereClauseOuterConditions = new StringBuilder(256);
        String havingClause = "";

        if (CollectionUtils.isNotEmpty(columnNameEnumFilterList) && CollectionUtils.isNotEmpty(filterTypeEnumList) &&
            CollectionUtils.isNotEmpty(compareTypeEnumList) && CollectionUtils.isNotEmpty(filterValueList))
        {
            Boolean isWhereRequired = false;
            for (int i = 0; i < columnNameEnumFilterList.size(); i++)
            {
                if (StringUtils.equals(columnNameEnumFilterList.get(i).getValue(), ColumnNameEnum.ERROR_COUNT.getValue()))
                {
                    havingClause = buildHavingClause(columnNameEnumFilterList.get(i));
                }
                else
                {
                    whereClauseOuterConditions.append(buildWhereClauseOuterConditions(columnNameEnumFilterList.get(i)));
                    //never reaches array out of bound
                    if (i < (columnNameEnumFilterList.size() - 1) &&
                        !StringUtils.equals(columnNameEnumFilterList.get(i + 1).getValue(), ColumnNameEnum.ERROR_COUNT.getValue()))
                    {
                        whereClauseOuterConditions.append(" AND");
                    }
                    isWhereRequired = true;
                }
            }
            if (isWhereRequired)
            {
                whereClauseOuter.append(" WHERE").append(whereClauseOuterConditions);
            }
        }
        return new Pair<>(havingClause, whereClauseOuter.toString());
    }
	
------------------------------------------------------------------------------------------

	INSERT INTO catco_owner.err_rec_dtl(
	err_roe_id, cat_rprtr_imid, actn_type, firm_roe_id, msg_type, odr_key_dt, odr_id, sym, prnt_odr_key_dt, prnt_odr_id, orgng_imid, event_ts, side, pr, qty, min_qty, odr_type, tif, trdg_sssn, hndlg_instr, seq_nb, ats_dsply_ind, dsply_pr, wrkng_pr, dsply_qty, nbb_pr, nbb_qty, nbo_pr, nbo_qty, nbbo_src, nbbo_ts, cncl_qty, lvs_qty, rsrvd_for_fut_use, prior_odr_key_dt, prior_odr_id, fill_key_dt, flmnt_id, prior_fill_key_dt, prior_flmnt_id, mnl_fl, elctc_ts, cpcty, flmnt_link_type, clnt_dtl, firm_dtl, dept_type, rcvng_desk_type, info_barr_id, elctc_dplct_fl, mnl_odr_key_dt, mnl_odr_id, cstmr_dsply_ind_fl, firm_dsgnt_id, accnt_hldr_type, afflt_fl, agrtd_odrs, ngttd_trd_fl, rep_ind, ats_odr_type, quote_key_dt, quote_id, prior_quote_key_dt, prior_quote_id, sndr_imid, dstnt, rtd_quote_id, only_one_quote_fl, bid_pr, bid_qty, ask_pr, ask_qty, usltd_ind, mp_stts_cd, quote_rjctd_fl, rcvr_imid, sndr_type, rtd_odr_id, iso_ind, inttr, cncl_fl, cncl_ts, dstnt_type, sssn, route_rjctd_fl, dup_roid_cond, trd_key_dt, trd_id, tape_trd_id, mkt_cntr_id, side_dtl_ind, buy_dtl, sll_dtl, rptg_xcptn_cd, rcvd_quote_id, quote_wntd_ind, optn_id, opn_cls_ind, prior_unlkd, next_unlkd, exch_orgn_cd, ats_odr_type_elmnt, raw_data, crtd_ts)
	VALUES (500001, 'A', 'action1', '60001', 'type1', '2020-02-21', 'order1', 'symbol1', 'parentOrderKey1', 'parentOrderId1', 'orgImid1', '2020-08-21 07:36:44', 'side1', 'pr1', '1', '1', 'orderTyp1', 'tif1', 'session1', 'handling1', 'seq1', 'atsD1', 'dsp1', 'w1', '1', 'nbb1', '1', 'nbo1', 1, 'src1', '2020-08-21 06:36:44', '1', '1', '1', '2020-07-01', 'priorOrdId1', '2020-05-01', 'flmn1', '2020-03-01', 'priorFlm1', 'mnl1', '2020-08-21 08:36:44', '1', 'link1', 'clnt1', 'firmDtl1', 'dept1', 'rvbng1', 'infoBar1', 'elctc1', '2020-08-01', 'mnl1', 'fl1', 'firmDsg1', 'accHldr1', 'aff1', 'agrOrd1', 'ngtdd1', 'rep1', 'atsOdr1', '2020-05-01', 'quote1', '2020-05-01', 'prrQo1', 'sndr1', 'dstnt1', 'rtd1', 'only1', 'bid1', '1', 'ask1', '1', 'usltd1', 'mp1', 'quote1', 'rcvr1', 'sndrTy1', 'rtdOdr1', 'iso1', 'int1', 'cnclFl1', '2020-08-21 08:36:44', 'dstnType1', 'sesson1', 'route1', 'dup1', '2020-04-01', 'trd1', 'tape1', 'mktCnt1', 'side1', 'buy1', 'sll1', 'rptg1', 'rcvd1', 'quoteWind1', 'optnId1', 'opnCls1', 'prUnl1', 'nxtUnl1', 'exch1', 'atsElem1', 'raw1', '2020-08-21 07:36:44');
			

------------------------------------------------------------------------------------------

INSERT INTO catco_owner.err_rec_info(
	err_roe_id, cat_rprtr_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, file_nm, file_frmt, prcsg_dt, trd_dt, event_type, prdct_type, rprd_st, err_type, actn_type, firm_roe_id, err_list, msg_type, crtd_ts, crtd_by, updtd_ts, updtd_by, rpr_type_cd, rpr_ts, rpr_user_id, crctn_due_ts)
	VALUES ('500001', 22, 'A', 22, 123, 'fileName', 'fileformat', '2019-01-01', '2019-01-01', 'MENO', 'Options', 'R', 'Rejection', 'aaaa', '20191218_101-MONO-No-Error', 'errList', 'messag', '2019-08-22 11:36:44', 'user1', '2019-08-22 11:36:44', 'user2', 'Corrected1', '2019-11-11', 'dummyRepair1', '2019-12-22 11:36:44');
	

-------------------------------------------------------------------------------------------
SELECT err_cd, err_ds, grp_rpr_elgbl, rec_fld, crtd_ts, crtd_by, updtd_ts, updtd_by
	FROM catdd_owner.err_cd_lk;
	
INSERT INTO catdd_owner.err_cd_lk(
	err_cd, err_ds, grp_rpr_elgbl, rec_fld, crtd_ts, crtd_by, updtd_ts, updtd_by)
	VALUES (1011, 'Some error message', true, 'aaaa', '2019-04-04', 'user1', '2019-04-04', 'user2');
	
-------------------------------------------------------------------------------------------
SELECT err_roe_id, err_cd, err_value, crtd_ts, crtd_by, updtd_ts, updtd_by
	FROM catdd_owner.err_rec_cd;
	
INSERT INTO catdd_owner.err_rec_cd(
	err_roe_id, err_cd, err_value, crtd_ts, crtd_by, updtd_ts, updtd_by)
	VALUES ('15300', 2001, 'Missing or Invalid accountHolderType', '2019-04-04', 'user1', '2019-04-04', 'user2');
	
delete from catdd_owner.err_rec_cd where err_roe_id='12353';
-------------------------------------------------------------------------------------------

SELECT err_roe_id, cat_rprtr_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, file_nm, file_frmt, prcsg_dt, trd_dt, event_type, prdct_type, rprd_st, err_type, actn_type, firm_roe_id, err_list, msg_type, crtd_ts, crtd_by, updtd_ts, updtd_by
	FROM catdd_owner.err_rec_info;

Delete from catdd_owner.err_rec_info where err_roe_id='12353';

INSERT INTO catdd_owner.err_rec_info(
	err_roe_id, cat_rprtr_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, file_nm, file_frmt, prcsg_dt, trd_dt, event_type, prdct_type, rprd_st, err_type, actn_type, firm_roe_id, err_list, msg_type, crtd_ts, crtd_by, updtd_ts, updtd_by)
	VALUES ('15300', 79, 'SUNNY', 79, 123, 'fileName', 'fileformat', '2019-01-01', '2019-01-01', 'MENO', 'Options', 'P', 'Rejection', 'aaaa', '20191205_101-MONO-No-Error', 'errList', 'messag', '2019-08-22 11:36:44', 'user1', '2019-08-22 11:36:44', 'user2');
-------------------------------------------------------------------------------------------
SELECT pndng_rec_id, cat_rprtr_id, cat_rprtr_imid, cat_sbmtr_id, sbmtr_3rd_prty, actn_type, err_roe_id, firm_roe_id, msg_type, odr_key_dt, odr_id, sym, prnt_odr_key_dt, prnt_odr_id, orgng_imid, event_ts, side, pr, qty, min_qty, odr_type, tif, trdg_sssn, hndlg_instr, seq_nb, ats_dsply_ind, dsply_pr, wrkng_pr, dsply_qty, nbb_pr, nbb_qty, nbo_pr, nbo_qty, nbbo_src, nbbo_ts, cncl_qty, lvs_qty, rsrvd_for_fut_use, prior_odr_key_dt, prior_odr_id, fill_key_dt, flmnt_id, prior_fill_key_dt, prior_flmnt_id, mnl_fl, elctc_ts, cpcty, flmnt_link_type, clnt_dtl, firm_dtl, dept_type, rcvng_desk_type, info_barr_id, elctc_dplct_fl, mnl_odr_key_dt, mnl_odr_id, cstmr_dsply_ind_fl, firm_dsgnt_id, accnt_hldr_type, afflt_fl, agrtd_odrs, ngttd_trd_fl, rep_ind, ats_odr_type, quote_key_dt, quote_id, prior_quote_key_dt, prior_quote_id, sndr_imid, dstnt, rtd_quote_id, only_one_quote_fl, bid_pr, bid_qty, ask_pr, ask_qty, usltd_ind, mp_stts_cd, quote_rjctd_fl, rcvr_imid, sndr_type, rtd_odr_id, iso_ind, inttr, cncl_fl, cncl_ts, dstnt_type, sssn, route_rjctd_fl, dup_roid_cond, trd_key_dt, trd_id, tape_trd_id, mkt_cntr_id, side_dtl_ind, buy_dtl, sll_dtl, rptg_xcptn_cd, rcvd_quote_id, quote_wntd_ind, optn_id, opn_cls_ind, prior_unlkd, next_unlkd, exch_orgn_cd, ats_odr_type_elmnt, rprd_st, crtd_by, crtd_ts, updtd_by, updtd_ts, rec_rprtr_imid
	FROM catdd_owner.pndng_rec;
	
SELECT count(*) from information_schema.columns where table_name='pndng_rec';

SELECT column_name from information_schema.columns where table_name='pndng_rec' order by column_name asc;

delete from catdd_owner.pndng_rec where pndng_rec_id=33

update catdd_owner.pndng_rec set err_roe_id=15200 where pndng_rec_id=35


-------------------------------------------------------------------------------------------
package org.finra.catdd.service.impl;

import java.time.LocalDate;
import java.time.LocalTime;
import java.util.List;

import org.apache.commons.lang3.StringUtils;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import org.springframework.util.Assert;

import org.finra.catdd.common.enums.DateTypeEnum;
import org.finra.catdd.common.enums.PerspectiveEnum;
import org.finra.catdd.common.enums.ProductTypeEnum;
import org.finra.catdd.common.helper.PerspectiveHelper;
import org.finra.catdd.common.utils.CatddDateUtils;
import org.finra.catdd.dao.ErrorSummaryDao;
import org.finra.catdd.dao.HerdDao;
import org.finra.catdd.dao.helper.DateHelper;
import org.finra.catdd.model.dto.ErrorSummaryDetail;
import org.finra.catdd.model.dto.ErrorSummaryDetails;
import org.finra.catdd.model.dto.ErrorSummaryPostRequest;
import org.finra.catdd.model.dto.ErrorSummaryRequest;
import org.finra.catdd.model.dto.ErrorSummaryResponse;
import org.finra.catdd.service.ErrorSummaryService;

/**
 * The implementation class for {@link ErrorSummaryService}
 */
@Service
public class ErrorSummaryServiceImpl implements ErrorSummaryService
{
    @Autowired
    private ErrorSummaryDao errorSummaryDao;

    @Autowired
    private PerspectiveHelper perspectiveHelper;

    @Autowired
    private HerdDao herdDao;

    @Autowired
    private DateHelper dateHelper;

    @Value("${processingDate.cutOff.hour}")
    private Integer processingDateCutOffHour;

    @Value("${processingDate.cutOff.minute}")
    private Integer processingDateCutOffMinute;

    @Value("${processingDate.cutOff.second}")
    private Integer processingDateCutOffSecond;

    @Value("${processingDate.cutOff.nanoOfSecond}")
    private Integer processingDateCutOffNanoOfSecond;

    @Override
    public ErrorSummaryResponse getErrorSummary(ErrorSummaryPostRequest errorSummaryPostRequest)
    {
        Assert.notNull(errorSummaryPostRequest, "The error summary request must not be null.");
        Assert.notNull(errorSummaryPostRequest.getStartRow(), "The error summary parameter \"startRow\" must be specified.");
        Assert.notNull(errorSummaryPostRequest.getEndRow(), "The error summary parameter \"endRow\" must be specified.");
        Assert.notNull(errorSummaryPostRequest.getErrorSummary(), "The error summary filter parameters must not be null.");
        ErrorSummaryRequest errorSummaryRequest = errorSummaryPostRequest.getErrorSummary();
        Assert.notNull(errorSummaryRequest.getCatOrganizationId(), "The parameter \"catOrganizationId\" must be specified.");

        DateTypeEnum dateTypeEnum =
            StringUtils.isEmpty(errorSummaryRequest.getDateType()) ? DateTypeEnum.PROCESSING_DATE : DateTypeEnum.find(errorSummaryRequest.getDateType());

        ProductTypeEnum productTypeEnum =
            StringUtils.isEmpty(errorSummaryRequest.getProductType()) ? ProductTypeEnum.ALL : ProductTypeEnum.find(errorSummaryRequest.getProductType());

        PerspectiveEnum perspectiveEnum = perspectiveHelper.getPerspectiveEnum(errorSummaryRequest.getPerspective());

        validateDate(errorSummaryRequest.getDate());
        String date = errorSummaryRequest.getDate();

        if (StringUtils.isEmpty(date))
        {
            date = dateHelper.getDate(dateTypeEnum, LocalDate.now(), LocalTime.now(),
                LocalTime.of(processingDateCutOffHour, processingDateCutOffMinute, processingDateCutOffSecond, processingDateCutOffNanoOfSecond));
        }

        ErrorSummaryResponse errorSummaryResponse = new ErrorSummaryResponse();
        ErrorSummaryDetails errorSummaryDetails = new ErrorSummaryDetails();
        errorSummaryResponse.setErrorSummaryDetails(errorSummaryDetails);
        errorSummaryDetails.setCatOrganizationId(errorSummaryRequest.getCatOrganizationId());
        errorSummaryDetails.setProductType(productTypeEnum.getValue());
        errorSummaryDetails.setPerspective(perspectiveEnum.name());
        errorSummaryDetails.setDateType(dateTypeEnum.getValue());
        errorSummaryDetails.setDate(date);

        List<ErrorSummaryDetail> errorSummaryDetailList = errorSummaryDao
            .getErrorSummaryDetails(errorSummaryRequest.getCatOrganizationId(), dateTypeEnum, date, productTypeEnum, perspectiveEnum,
                errorSummaryPostRequest.getStartRow(), errorSummaryPostRequest.getEndRow());
        errorSummaryDetails.setErrorSummaryDetails(errorSummaryDetailList);

        errorSummaryResponse.setLastRow(errorSummaryDetailList.size());
        return errorSummaryResponse;
    }

    /**
     * Validate the date
     *
     * @param dateValue the date value
     */
    protected void validateDate(String dateValue)
    {
        CatddDateUtils.getDateFromStringYYYYMMDD(dateValue);
    }

//    /**
//     * Method that returns date for processing or trade date based on the login time and date of the user
//     *
//     * @param currentTime current time when the user is requesting the error summary
//     * @param cutOffTime cutoff time to decide the processing date for the error summary
//     *
//     * @return the date to be used to get error summary
//     */
//    protected String getDate(DateTypeEnum dateTypeEnum, LocalDate currentDate, LocalTime currentTime, LocalTime cutOffTime)
//    {
//        Instant currentDateInstant = currentDate.atStartOfDay(ZoneId.of(JSON_TIMESTAMP_TIMEZONE)).toInstant();
//
//        String startExpectedPartitionValue = CatddDateUtils
//            .getDateAsStringYYYYMMDD(Date.from(currentDate.minusDays(DAYS_FOR_PARTITION_VALUE).atStartOfDay(ZoneId.of(JSON_TIMESTAMP_TIMEZONE)).toInstant()));
//
//        String endExpectedPartitionValue = CatddDateUtils.getDateAsStringYYYYMMDD(Date.from(currentDateInstant));
//
//        ExpectedPartitionValuesInformation expectedPartitionValuesInformation =
//            herdDao.getExpectedPartitionValues(startExpectedPartitionValue, endExpectedPartitionValue);
//
//        // This list is expected to have more than 3 dates as the date range we are giving is 10 days
//        List<String> list = expectedPartitionValuesInformation.getExpectedPartitionValues();
//
//        return Objects.equals(dateTypeEnum, DateTypeEnum.PROCESSING_DATE) ? getProcessingDate(currentDateInstant, currentTime, cutOffTime, list) :
//            getTradeDate(currentDateInstant, list);
//    }
//
//    /**
//     * Method that returns the processing date as per the business requirement
//     *
//     * @param currentDateInstant current date instant with Eastern timezone
//     * @param currentTime current time when the user is requesting the error summary
//     * @param cutOffTime cutoff time to decide the processing date for the error summary
//     * @param list the list of trade dates from today-10 days till today
//     *
//     * @return the processing date
//     */
//    protected String getProcessingDate(Instant currentDateInstant, LocalTime currentTime, LocalTime cutOffTime, List<String> list)
//    {
//        String processingDate;
//
//        if (CatddDateUtils.getDateFromStringYYYYMMDD(list.get(list.size() - 1)).before(Date.from(currentDateInstant)))
//        {
//            processingDate = list.get(list.size() - 2);
//        }
//        else
//        {
//            processingDate = currentTime.isBefore(cutOffTime) ? list.get(list.size() - 3) : list.get(list.size() - 2);
//        }
//
//        return processingDate;
//    }
//
//    /**
//     * Method that returns the trade date as per the business requirement
//     *
//     * @param currentDateInstant current date instant with Eastern timezone
//     * @param list the list of trade dates from today-10 days till today
//     *
//     * @return the processing date
//     */
//    protected String getTradeDate(Instant currentDateInstant, List<String> list)
//    {
//        return CatddDateUtils.getDateFromStringYYYYMMDD(list.get(list.size() - 1)).before(Date.from(currentDateInstant)) ? list.get(list.size() - 1) :
//            list.get(list.size() - 2);
//    }
}

--------------------------------------------------------------------------


SELECT date, cat_rprtr_imid catImid, cat_sbmtr_id catSubmitterId, rprd_st repairedStatus, err_type errorType, err_cd errorCode, err_ds errorReason,
 COUNT(*) errorCount FROM (SELECT erd.trd_dt date, cat_rprtr_imid, cat_sbmtr_id, rprd_st, err_type, erc.err_cd, ecl.err_ds FROM catdd_owner.err_rec_info eri,
 catdd_owner.err_rec_cd erc, catdd_owner.err_cd_lk ecl WHERE  eri.prdct_type=:productType AND (eri.cat_rprtr_id=:orgId OR eri.cat_sbmtr_id=:orgId) AND
 (eri.trd_dt <= :endDate AND eri.trd_dt >= :startDate) AND eri.err_rec_info_id = erc.err_rec_info_id AND ecl.err_cd= erc.err_cd AND eri.rprd_st='R'
 AND NOT EXISTS (SELECT err_roe_id FROM catdd_owner.pndng_rec_info pri WHERE pri.err_roe_id=eri.err_roe_id) UNION ALL SELECT erd.trd_dt date,
 eri.cat_rprtr_imid, eri.cat_sbmtr_id, eri.rprd_st, eri.err_type, erc.err_cd, ecl.err_ds FROM catdd_owner.err_rec_info eri, catdd_owner.err_rec_cd erc,
 catdd_owner.err_cd_lk ecl, catdd_owner.pndng_rec_info pri WHERE  eri.prdct_type=:productType AND (eri.cat_rprtr_id=:orgId OR eri.cat_sbmtr_id=:orgId)
 AND (eri.trd_dt <= :endDate AND eri.trd_dt >= :startDate) AND eri.err_rec_info_id = erc.err_rec_info_id AND ecl.err_cd= erc.err_cd AND
 eri.err_roe_id = pri.err_roe_id AND eri.rprd_st='R' AND pri.rprd_st='S' UNION ALL SELECT erd.trd_dt date, pri.cat_rprtr_imid, pri.cat_sbmtr_id,
 pri.rprd_st, pri.err_type, erc.err_cd, ecl.err_ds FROM catdd_owner.err_rec_info eri, catdd_owner.err_rec_cd erc, catdd_owner.pndng_rec_info pri,
 catdd_owner.err_cd_lk ecl WHERE  eri.prdct_type=:productType AND (eri.cat_rprtr_id=:orgId OR eri.cat_sbmtr_id=:orgId) AND (eri.trd_dt <= :endDate AND eri.trd_dt >= :startDate) AND
 eri.err_rec_info_id = erc.err_rec_info_id AND (eri.rprd_st='U' OR (eri.rprd_st='R' AND pri.rprd_st='P'))  AND eri.err_roe_id = pri.err_roe_id AND
 ecl.err_cd= erc.err_cd UNION ALL SELECT erd.trd_dt date, cat_rprtr_imid, cat_sbmtr_id, rprd_st, err_type, erc.err_cd, ecl.err_ds
 FROM catdd_owner.err_rec_info eri, catdd_owner.err_rec_cd erc, catdd_owner.err_cd_lk ecl      WHERE  eri.prdct_type=:productType AND
 (eri.cat_rprtr_id=:orgId OR eri.cat_sbmtr_id=:orgId) AND (eri.trd_dt <= :endDate AND eri.trd_dt >= :startDate) AND eri.err_rec_info_id = erc.err_rec_info_id AND eri.rprd_st='U'
 AND ecl.err_cd= erc.err_cd AND NOT EXISTS (SELECT err_roe_id FROM catdd_owner.pndng_rec_info pri WHERE pri.err_roe_id=eri.err_roe_id))
 esr GROUP BY date, cat_rprtr_imid, cat_sbmtr_id, err_type, rprd_st, err_cd, err_ds ORDER BY date, cat_rprtr_imid, cat_sbmtr_id, err_type, rprd_st, err_cd LIMIT 20 OFFSET 1
 
 
 --------------------------------------------------------------------------


Add a logger in Credstash and GetBusinessObjectData

so that retires message would make sense




if the attribute is Not thirdParty or reporterOrgId, and attibute is not present, throw Exception("no value found for required attribute")

if the attribute is Not thirdParty or reporterOrgId, but targetFileName and formatUsage="INT_META", check for resultAttribute.
		If result attibute is Failure, file name should have been present,so throw Exception("no value found for required attribute")
		Else if result attribute is Not Failure, set fileName as null ? 


---------------------------------------------------------------------------------
INSERT INTO catdd_owner.herd_info_lk(
	herd_info_id, actn_type, strg_name, name_space, bus_objct_dfntn, usage_cd, file_type, frmt_vrsn, prtn_key)
	VALUES (2, 'FDBK_DWNLD', 'S3_MANAGED', 'IndustryMember', 'ORDER_EVENTS', 'INT', 'TXT', 0, ?);
	
---------------------------------------------------------

getMessageType -> make a db call, check against businessObjectdata, if present then return new FeedbackProcessor() else return NoOpProcessor().
					--------------> Create a connection, get businessObjectdata, compare. 
				
--------------------------------------------------------------------------------
 @Test
    @Tag("api")
    @Tag("feedback")
    @Publish(name = "Verify roles for getting feedback file details", description = "User and Readonly roles should able to get feedback file details.", coverage = "CATDD-405")
    public void testRoleForgetFeedbackFiles()
    {
        Role role = Role.CAT_PRIV_CATDD_IM_USER;

        Map<String, Object> queryParam = new HashMap<>();
        queryParam.put(QueryParams.CAT_RPRTR_ORG_ID.getValue(), role.getOrgId());
        queryParam.put(QueryParams.START_DATE.getValue(), "");
        queryParam.put(QueryParams.END_DATE.getValue(), "");
        queryParam.put(QueryParams.PERSPECTIVE.getValue(), Perspective.SUBMITTER);

        FeedbackBuilder builder = FeedbackBuilder.build().getFeedbackFiles(queryParam, role.getRole());
        FeedbackFileDetailsOutput actualFeedbackFileDetails = builder.getFeedbackFileDetailsOutput();

        List<FeedbackFileInformationEntity> feedbackFileInformationEntities = feedBackDao.getByQueryParams(queryParam);
        FeedbackFileDetailsOutput expectedFeedbackFileDetails = buildFeedbackFileOutput(feedbackFileInformationEntities, queryParam);
        assertEquals(actualFeedbackFileDetails, expectedFeedbackFileDetails, "Feedback file details are mismatching");
    }

    private FeedbackFileDetailsOutput buildFeedbackFileOutput(List<FeedbackFileInformationEntity> feedbackFileInformationEntityList, Map<String, Object> queryParams)
    {
        List<FeedbackFileDetailOutput> feedbackFileDetailOutputList = feedbackFileInformationEntityList.stream()
                .map((feedbackFileInformationEntity -> convertFeedbackFileInformationEntityToFeedbackFileDetailOutput(feedbackFileInformationEntity)))
            .collect(Collectors.toList());

        FeedbackFileDetailOutput[] feedbackFileDetailOutputs = new FeedbackFileDetailOutput[feedbackFileDetailOutputList.size()];
        feedbackFileDetailOutputs = feedbackFileDetailOutputList.toArray(feedbackFileDetailOutputs);

        FeedbackFileDetailsOutput feedbackFileDetailsOutput = FeedbackFileDetailsOutput.builder().catOrganizationId((Integer) queryParams.get(QueryParams.CAT_RPRTR_ORG_ID.getValue()))
            .startDate((String) queryParams.get(QueryParams.START_DATE.getValue()))
            .endDate((String) queryParams.get(QueryParams.END_DATE.getValue()))
            .perspective((String) queryParams.get(QueryParams.PERSPECTIVE.getValue()))
            .feedbackFileDetailOutputs(feedbackFileDetailOutputs)
            .build();

        return feedbackFileDetailsOutput;
    }

---------------------------------------------------------------------------------------------------------------
SQS message example:
{
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
			"{  \"message\" : \"{ \\\"eventDate\\\" : \\\"\\\", \\\"businessObjectDataKey\\\" : { \\\"namespace\\\" : \\\"IndustryMember\\\", \\\"businessObjectDefinitionName\\\" : \\\"ORDER_EVENTS\\\", \\\"businessObjectFormatUsage\\\" : \\\"ING\\\", \\\"businessObjectFormatFileType\\\" : \\\"TXT\\\", \\\"businessObjectFormatVersion\\\" : 0, \\\"partitionValue\\\" : \\\"2019-10-15\\\",\\n \\\"subPartitionValues\\\" : [\"1331_MOTF_20191015_testFilePair_OrderEvents_144443\"] }, \\\"newBusinessObjectDataStatus\\\" : \\\"VALID\\\" , \\\"oldBusinessObjectDataStatus\\\" : \\\"\\\" }\",  \"type\" : \"\",  \"signature\" : \"\",  \"timestamp\" : \"\",\"topicArn\" : \"\",\"messageId\" : \"\",  \"signatureVersion\" : \"1\",  \"unsubscribeURL\" : \"\",  \"signingCertURL\" : \"\"}",
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        }
        
        ]
}
--------------------------------------------------------------------------------------------------------------------
file format and compression, can it be uppercase?

1. Create the file in db table with info that we have at hand.  ***
2. Validation....
		File name format check ***
		File name should not exceed 90 characters ***
		Check if the submitter id matches fip logged in user ***
		File format check(for both meta and data files)... json,csv ***
		Meta files has .meta before file format ***
		File compression type check(.bz2) ***
		
		Do we need to verify imid??? NO ***
		
		File upload size(<1gb) 
		************Couldnt upload more than 10 files********UI part
		************Total size of file <5gb********UI part
		
3. Call DM with herd_info data to get the s3 location
4. upload the file
5. Register to DM


https://stackoverflow.com/questions/336210/regular-expression-for-alphanumeric-and-underscores
--------------------------------------------------------------------------------------------------------------------
if part size is 5MB

Lets say I use 3 Streams,

10000/3 = 3333 ....... Each stream can have upto 3333*5= 16 GB

is the given stream file size divided into 10000 parts....?



Portal -> Endpoint for upload with file			-> store details in db
												-> server-to-s3 -> response from s3
------------------------------------------------------------------------------------------------------------------------

If the submitter from existing is used as thrdparty or submitter in another firm, restrict


		Before                                                       After

1. If existing record(based on imid),						If existing record(based on imid),
	check(new submitter = old submitter)						check(new submitter = old submitter)
	and check(new thrdparty = old thrdparty)						or check(new thrdparty = old submitter)
	Exception if true													Exception if true                          If existing record(based on imid)
																													check(new submitter = old thrdparty)
																														or check(new thrdparty = old thrdparty)	
																															Exception if true


			
Same submitter, different third party........ or no third-party(in both existing and new)........or new has third-party, existing doesnt
check(new Expiration date > old expiration date) -> Third Party relationship can not exist beyond submitter relationship


					New																			Existing
orgId				79																				7059(same org, response -> 1), 79(diff org, response -> 2)
imid				ABC																				ABC
Submitter			999																				999		
third				111																				222
effective			10-04-2019																	09-04-2019
expiration			10-25-2019																	10-20-2019

					Response	1.Third Party relationship can not exist beyond submitter relationship
				OR	Response	2.IMID/Submitter Relationship Already Exits. Please Contact FINRA CAT Help Desk %S for Assistance. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------

					New																			Existing
orgId				79																				7059(same org, response -> 1), 79(diff org, response -> 2)
imid				ABC																				ABC				
Submitter			999																				999			
third																								
effective			10-04-2019																	09-04-2019
expiration			10-25-2019																	10-20-2019
				
					Response	1.Relationship Already Exists
				OR	Response	2.IMID/Submitter Relationship Already Exits. Please Contact FINRA CAT Help Desk %S for Assistance. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------

					New																			Existing(same org, response -> 1), 79(diff org, response -> 2)
orgId				79																				7059
imid				ABC																				ABC		
Submitter			999																				999			
third				123																				123
effective			10-04-2019																	09-04-2019
expiration			10-25-2019																	10-20-2019

					Response	1.Relationship Already Exists
				OR	Response	2.IMID/Submitter Relationship Already Exits. Please Contact FINRA CAT Help Desk %S for Assistance. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------

					New																			Existing
orgId				79																				7059
imid				ABC																				ABC
Submitter			999																				999
third				111																				
effective			10-04-2019																	09-04-2019
expiration			10-25-2019																	10-20-2019

							Third Party relationship can not exist beyond submitter relationship
				OR			IMID/Submitter Relationship Already Exits. Please Contact FINRA CAT Help Desk %S for Assistance.
-----------------------------------------------------------------------------------------------------------------------------------------------------

Same submitter, new has no third-party, existing has third-party
check(new Expiration date < old expiration date) -> Expiring Submitter relationship while Third Party relationship exists beyond submitter relationship 
																												OR IMID/Submitter Relationship...........
				New																			Existing
orgId				79																				88
imid				ABC																				ABC
Submitter			999																				999
third																								222
effective			10-04-2019																	09-04-2019
expiration			10-18-2019																	10-20-2019
	
			
			
If(orgId_old = org_idnew & submitter_old = submitter_new   AND  (third-party_old=null & third-party_new!=null) ->call checker...adding child...
If(orgId_old = org_idnew & submitter_old = submitter_new   AND  (third-party_old!=null & third-party_new=null) ->call checker...adding parent...

If(submitter_old = submitter_new   AND  (third-party_old=abc & third-party_new=def OR newthirdpaty=oldthirdparty) ->new exception...


--------------------------------------------------------------------
File_submission table	-> no change

Herd_info_lookup_table		-> removed constraint

status_lookup_table		-> added a grant

feedback_table	-> added

herd_sequence -> added

feedback_sequence -> added

----------------------------------------------------
File uploaded at 1pm, shows 6pm. because when I just save the local time, it saves in utc,

-----------------------------------------------------

1. Add all feedbacks to a list. then execute send the list. After connection, add a for loop or stream the list and after that commit. ***

2. When we add to the herd_table what is the partition key gonna be? only compare with the rest except partition key ???	***

3. Where is the return ignore.name() enum going to

4. Sequence naming.... Sequence review...... Add into.....	***

5. remove propertyUtility and lambdaTest	***

6. redundant true???? controller ***

7. GetHerdInfo() instead of checkHerdInfo(). Rename in lambda, and java doc too.

https://wiki.finra.org/display/DataManagement/Business+Object+Data+Status+Change+Event+Notification

---------------------------------------------------------------------------------------------
File upload:

1. ApacheCommons stream to disk, FileItem coverted to file, then transferManager to upload file to s3 -> FAIL in the middle for 1gb file, OK for smaller files. Multipart(YES)
2. ApacheCommons stream to disk, then transferManager to upload stream to s3 -> 1gb file (Avg upload time : 2:44) , Multipart(YES), Thread not seem to be having effect
	2:39, 2:30, 2:54, 2:51

3. ApacheCommons stream to server then transferManager to upload stream to s3(No content length) -> 1gb file (Avg upload time : 2:28) , SinglePart
	2:08, 2:46, 2:23, 2:36
4. ApacheCommons stream to server then transferManager to upload stream to s3(Yes content length) -> 1gb file (Avg upload time : 2:01) , SinglePart
	2:18, 1:50, 1:56, 2:02
		
	Came to a conclusion that some ways above using multipart because of TRANSFER_PART_COMPLETED_EVENT log in progressListener
	
	Questions:
	1. How do we verify if it is actually using a multipart upload?
	2. Best way to get the content-length for metadata when we are streaming to s3 using transferManager?
	3. Use streaming using apache commons and store in an EFS, then put to s3. Using stream does it do multipart? or using the file?
	
-------------------------------------------------------------------------------------------------------------------------------------------------------------

1.   Client -> Server (Apache Commons streaming)         &       Server -> s3 (Transfer Manager - singlePart - Streaming)

		fileSize|   Wifi	| Ethernet  |Content-length
		  1 GB	|			|	 :31    |	NO
				|			|	 :28    |		
				|			|	 :26    |		
				|			|	 :28    |		
	  -----------------------------------------------
				|			|	 :52    |	YES
				|			|	1:06    |		
				|			|	 :51    |	
						
   -> Content-length can only be known if client supplies it as a header but we want to avoid that.
   -> Another way to get the content length is to buffer the stream into the memory and get the size which defeats the whole purpose of streaming.   

******************************************************************************************************************************************************************

2. Client -> Server (Apache commons file upload)           Save to a disk or EFS			Server -> S3 (Transfer manager  -> Stream or file)
	
	a)upload STREAM from server to S3
															 |}
		   fileSize    	|		    Wifi(timeToUpload)		 |}    EthernetCable(timeToUpload)   |
						----------------------------------------------------------------------------------------
						| Client-Server | Server-S3	| total	 |} Client-Server |  Server-S3   | total
		------------------------------------------------------------------------------------------------------
		  1 GB			|	 	0:25	|	2:33	|	2:58 |}		0:24 	  |		0:52     | 1:16		
						|		0:25	|	2:11	|	2:36 |}		0:25 	  | 	1:15     | 1:40
						|				|			|		 |}		0:25  	  |		1:07     | 1:32
						|				|			|		 |}		0:24  	  |		1:04     | 1:28		
															 |}
						
	b)upload FILE from server to S3
													 |}
		   fileSize    	|		    Wifi(timeToUpload)		 |}    EthernetCable(timeToUpload)   |
						----------------------------------------------------------------------------------------
						| Client-Server | Server-S3	| total	 |} Client-Server |  Server-S3   | total
		------------------------------------------------------------------------------------------------------
		  1 GB			|	 			|			|		 |}		0:24 	  |		0:18     | 0:42		
						|				|			|		 |}		0:24 	  | 	0:18     | 0:42
						|				|			|		 |}		0:23  	  |		0:19     | 0:42
						|				|			|		 |}		0:24  	  |		0:19     | 0:43		
															 |}
	
  
   -> Reasonably the best way to do file upload. People use Lambdas to achieve this
   -> On average, to upload a 1gb file it takes about --> Streaming(1 minutes 58 seconds,1:56, 2:11 | Ethernet: 1:26,1:19, 1:17; ), file(1 minutes 2 seconds, 1:09 | Ethernet: 43,51,42)                          		   
   -> temp file not deleted after upload had completed ???? have to explicitly do item.delete();
   -> Does Stream do multipart? or just file?
   
******************************************************************************************************************************************************************  
3. Client -> Server (Apache Commons streaming) 								   Server -> s3 (Transfer Manager - in chunks - multipart streaming)


????????????????????????????????????????????????



******************************************************************************************************************************************************************
4. Client -> server(storing in disk)..................Server->(s3)

Uploading 1 gb file on average takes : 50 seconds

-----------------------------------------------------------------------------------------------------------------------------
5. Store in memory if less than 20MB, else store it in a disk temporarily, then use transferManager to upload to s3(James)
			   fileSize    	|		    Wifi(timeToUpload)		 |}    EthernetCable(timeToUpload)   |
						----------------------------------------------------------------------------------------
						| Client-Server | Server-S3	| total	 |} Client-Server |  Server-S3   | total 
		------------------------------------------------------------------------------------------------------
		  1 GB			|	 			|			|		 |}		0:46 	  |		0:15     | 1:01		
						|				|			|		 |}		0:42 	  | 	0:16     | 0:58
						|				|			|		 |}		0:42  	  |		0:15     | 0:57
						|				|			|		 |}		0:43  	  |		0:14     | 0:57	

-----------------------------------------------------------------------------------------------------------------------------
Streaming...................store upto 5MB, mark the position, then upload that chunk, come back and store another chunk, repeat...

Client to server time, server to s3 time 

---------------------------------------------------------------------------------------------------------------------------------------------------------------
FIle name validation scenarios:

DATA FILE:

2022_MYID_20170101_FileGroup1_OrderEvents_000123.csv.bz2     	PASS																			***
2022_MYID_20170101_FileGroup1_OrderEvents_000123.json.bz2		PASS																			***
2022_MYID_20170101_FileGroup1_OrderEvents_000123.dav.bz2		FAIL -> File Format incorrect													***
2022_MYID_20170101_FileGroup1_OrderEvents_000123.csv.xx3		FAIL -> Compression type incorrect												***
2022_MYID_20170101_OrderEvents_000123.csv.bz2					PASS									(Optional group)						***
2022_MYID_20170101_OrderEvents_000123.json.bz2					PASS									(Optional group)						***
2022_MYID_20170101_OrderEvents_000123.dav.bz2					FAIL -> File Format incorrect													***
2022_MYID_20170101_OrderEvents_000123.csv.xx3					FAIL -> Compression type incorrect												***
1111_MYID_20170101_FileGroup1_OrderEvents_000123.csv.bz2		FAIL -> Unauthorized user														***
1111_MYID_20170101_OrderEvents_000123.csv.bz2					FAIL -> Unauthorized user				(Optional group)						***
2022_MYID_20170101_FileGroup1_000123.csv.bz2					FAIL -> File name format incorrect			(missing fileKind)					???
2022_MYID_20170101_FileGroup1_OrderEvents.csv.bz2				FAIL -> File name format incorrect			(missing fileNumber)				***
2022_MYID_FileGroup1_OrderEvents_000123.csv.bz2					FAIL -> File name format incorrect			(missing file generation date)		***
2222_20170101_FileGroup1_OrderEvents_000123.csv.bz2				FAIL -> File name format incorrect			(missing imid)						***
MYID_20170101_FileGroup1_OrderEvents_000123.csv.bz2				FAIL -> File name format incorrect			(missing orgid)						***
20170101_FileGroup1_OrderEvents_000123.csv.bz2					FAIL -> File name format incorrect			(missing orgid and imid)			***
2222_20170101_OrderEvents_000123.csv.bz2						FAIL -> File name format incorrect			(missing imid and group)			***
2222_20170101_FileGroup1_000123.csv.bz2							FAIL -> File name format incorrect			(missing imid and fileKind)			***		

META FILE:
2022_MYID_20170101_FileGroup1_OrderEvents_000123.meta.csv     	PASS																												****
2022_MYID_20170101_FileGroup1_OrderEvents_000123.meta.json		PASS																												****
2022_MYID_20170101_FileGroup1_OrderEvents_000123.meta.dav		FAIL -> File Format incorrect																						****
2022_MYID_20170101_FileGroup1_OrderEvents_000123.dell.csv		FAIL ->	**Tricky part.If no meta. It is like data file with dell format...file format incorrect						****
2022_MYID_20170101_OrderEvents_000123.meta.csv					PASS									(Optional group)															****
2022_MYID_20170101_OrderEvents_000123.meta.json					PASS									(Optional group)															****
2022_MYID_20170101_OrderEvents_000123.meta.dav					FAIL -> Format incorrect																							****
2022_MYID_20170101_OrderEvents_000123.dell.csv					FAIL -> **Tricky part.If no meta. It is like data file with dell format...file format incorrect						****
1111_MYID_20170101_FileGroup1_OrderEvents_000123.meta.csv		FAIL -> Unauthorized user               																			****
1111_MYID_20170101_OrderEvents_000123.meta.csv					FAIL -> Unauthorized user				(Optional group)															****
2022_MYID_20170101_FileGroup1_000123.meta.csv					FAIL -> File name format incorrect			(missing fileKind)														????
2022_MYID_20170101_FileGroup1_OrderEvents.meta.csv				FAIL -> File name format incorrect			(missing fileNumber)													****
2022_MYID_20170101_FileGroup1_OrderEvents.meta.csv				FAIL -> File name format incorrect			(missing file generation date)											****
2222_20170101_FileGroup1_OrderEvents_000123.meta.csv			FAIL -> File name format incorrect			(missing imid)															****
MYID_20170101_FileGroup1_OrderEvents_000123.meta.csv			FAIL -> File name format incorrect			(missing orgid)															****
20170101_FileGroup1_OrderEvents_000123.meta.csv					FAIL -> File name format incorrect			(missing orgid and imid)												****
2222_20170101_OrderEvents_000123.meta.csv						FAIL -> File name format incorrect			(missing orgid and group)												****
2222_20170101_FileGroup1_000123.meta.csv						FAIL -> File name format incorrect			(missing imid and fileKind)		
-----------------------------------------------------------------------------------------------------------------------------------
05/20/2019
* Implement REST endpoint to CREATE ATS Order Types:

      1.Expiration Date (required) - this must be a timestamp date
	Effective Date (required) - this must be a timestamp date
        ----------------------------------------------------------> No Timestamp, only date ????
      2.Lets say for IMID, if provided a special characters, how is the response gonna look like, error code, or msg?
      3.Same combination of AtsOrderType and IMID replaces the existing record.
	How should we do it?
